{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>HighBP</th>\n",
       "      <th>HighChol</th>\n",
       "      <th>CholCheck</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Smoker</th>\n",
       "      <th>Stroke</th>\n",
       "      <th>HeartDiseaseorAttack</th>\n",
       "      <th>PhysActivity</th>\n",
       "      <th>Fruits</th>\n",
       "      <th>...</th>\n",
       "      <th>NoDocbcCost</th>\n",
       "      <th>GenHlth</th>\n",
       "      <th>MentHlth</th>\n",
       "      <th>PhysHlth</th>\n",
       "      <th>DiffWalk</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Income</th>\n",
       "      <th>Diabetes_012</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.220930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.174419</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.151163</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.209302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105286</th>\n",
       "      <td>105286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250595</td>\n",
       "      <td>0.775591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.224409</td>\n",
       "      <td>0.806102</td>\n",
       "      <td>0.462599</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.775591</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.955118</td>\n",
       "      <td>0.728909</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105287</th>\n",
       "      <td>105287</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.243025</td>\n",
       "      <td>0.900157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099843</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.099843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.591654</td>\n",
       "      <td>0.619969</td>\n",
       "      <td>0.014263</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105288</th>\n",
       "      <td>105288</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.699485</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.330221</td>\n",
       "      <td>0.699485</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.300515</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424871</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.525043</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105289</th>\n",
       "      <td>105289</td>\n",
       "      <td>0.508129</td>\n",
       "      <td>0.508129</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250095</td>\n",
       "      <td>0.508129</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.491871</td>\n",
       "      <td>0.508129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372968</td>\n",
       "      <td>0.016396</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929733</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105290</th>\n",
       "      <td>105290</td>\n",
       "      <td>0.055528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055528</td>\n",
       "      <td>0.184110</td>\n",
       "      <td>0.944472</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.055528</td>\n",
       "      <td>0.055528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.944472</td>\n",
       "      <td>0.513882</td>\n",
       "      <td>0.229631</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.944472</td>\n",
       "      <td>0.495373</td>\n",
       "      <td>0.611106</td>\n",
       "      <td>0.007933</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>105291 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0    HighBP  HighChol  CholCheck       BMI    Smoker  Stroke  \\\n",
       "0                0  0.000000  0.000000   1.000000  0.174419  1.000000     0.0   \n",
       "1                1  0.000000  1.000000   1.000000  0.220930  1.000000     0.0   \n",
       "2                2  0.000000  0.000000   1.000000  0.174419  0.000000     0.0   \n",
       "3                3  0.000000  0.000000   1.000000  0.151163  1.000000     0.0   \n",
       "4                4  1.000000  1.000000   1.000000  0.209302  1.000000     0.0   \n",
       "...            ...       ...       ...        ...       ...       ...     ...   \n",
       "105286      105286  1.000000  1.000000   1.000000  0.250595  0.775591     0.0   \n",
       "105287      105287  1.000000  1.000000   1.000000  0.243025  0.900157     0.0   \n",
       "105288      105288  1.000000  0.699485   1.000000  0.330221  0.699485     0.0   \n",
       "105289      105289  0.508129  0.508129   1.000000  0.250095  0.508129     0.0   \n",
       "105290      105290  0.055528  1.000000   0.055528  0.184110  0.944472     0.0   \n",
       "\n",
       "        HeartDiseaseorAttack  PhysActivity    Fruits  ...  NoDocbcCost  \\\n",
       "0                        0.0      1.000000  1.000000  ...     0.000000   \n",
       "1                        1.0      1.000000  1.000000  ...     0.000000   \n",
       "2                        0.0      1.000000  0.000000  ...     0.000000   \n",
       "3                        0.0      1.000000  1.000000  ...     0.000000   \n",
       "4                        0.0      0.000000  0.000000  ...     0.000000   \n",
       "...                      ...           ...       ...  ...          ...   \n",
       "105286                   0.0      0.000000  0.000000  ...     0.224409   \n",
       "105287                   0.0      1.000000  0.099843  ...     0.000000   \n",
       "105288                   0.0      0.300515  1.000000  ...     0.000000   \n",
       "105289                   0.0      0.491871  0.508129  ...     0.000000   \n",
       "105290                   0.0      0.055528  0.055528  ...     0.944472   \n",
       "\n",
       "         GenHlth  MentHlth  PhysHlth  DiffWalk       Sex       Age  Education  \\\n",
       "0       0.250000  0.066667  0.000000  0.000000  0.000000  0.166667   0.600000   \n",
       "1       0.500000  0.000000  0.000000  0.000000  1.000000  0.583333   0.800000   \n",
       "2       0.500000  0.100000  0.000000  0.000000  0.000000  0.500000   0.600000   \n",
       "3       0.000000  0.033333  0.000000  0.000000  0.000000  0.500000   1.000000   \n",
       "4       0.500000  1.000000  0.000000  0.000000  0.000000  0.250000   0.600000   \n",
       "...          ...       ...       ...       ...       ...       ...        ...   \n",
       "105286  0.806102  0.462599  0.500000  1.000000  0.775591  0.750000   0.955118   \n",
       "105287  1.000000  1.000000  1.000000  0.099843  0.000000  0.591654   0.619969   \n",
       "105288  0.424871  0.000000  0.000000  0.000000  1.000000  0.525043   0.600000   \n",
       "105289  0.372968  0.016396  0.066667  0.000000  0.000000  0.500000   1.000000   \n",
       "105290  0.513882  0.229631  0.100000  0.000000  0.944472  0.495373   0.611106   \n",
       "\n",
       "          Income  Diabetes_012  \n",
       "0       0.000000           0.0  \n",
       "1       0.857143           2.0  \n",
       "2       0.714286           0.0  \n",
       "3       0.857143           0.0  \n",
       "4       1.000000           2.0  \n",
       "...          ...           ...  \n",
       "105286  0.728909           1.0  \n",
       "105287  0.014263           1.0  \n",
       "105288  1.000000           1.0  \n",
       "105289  0.929733           1.0  \n",
       "105290  0.007933           1.0  \n",
       "\n",
       "[105291 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/balanced_sclaer_dataset_diabetes.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop(['Unnamed: 0'], axis=1, inplace=True)\n",
    "data.isnull().sum().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data[\"Diabetes_012\"].values\n",
    "X = data.drop([\"Diabetes_012\"], axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((89497, 21), (15794, 21), (89497, 1), (15794, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# разбиваем данные\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# а теперь преобразуем обучающую выборку в объект Dataset\n",
    "train_ds = TensorDataset(torch.from_numpy(X_train).type(torch.float32), torch.from_numpy(y_train).type(torch.float32))\n",
    "# для загрузки данных в ходе обучения мы создаем объект DataLoader на основе объекта Dataset\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([256, 21]), torch.Size([256, 1]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_c, y_c = next(iter(train_dl))\n",
    "x_c.shape, y_c.shape\n",
    "# 128 - это указанный нами batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyClassifierModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyClassifierModel, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(21, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc5 = nn.Linear(32, 3)\n",
    "        # self.relu5 = nn.ReLU()\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x= self.relu3(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.fc5(x)\n",
    "        # y = self.relu5(y)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyClassifierModel(\n",
      "  (fc1): Linear(in_features=21, out_features=64, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=64, out_features=128, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc4): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (relu4): ReLU()\n",
      "  (fc5): Linear(in_features=32, out_features=3, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model=MyClassifierModel()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем функцию потерь\n",
    "loss = nn.NLLLoss()\n",
    "# настраиваем оптимизатор и передаем туда параметры модели\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\NLP_5_sem\\TorchClassifier.ipynb Cell 14\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP_5_sem/TorchClassifier.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(x_b)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/NLP_5_sem/TorchClassifier.ipynb#X16sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# вычисляем значение функции потерь\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/NLP_5_sem/TorchClassifier.ipynb#X16sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss_value \u001b[39m=\u001b[39m loss(outputs, y_b)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NLP_5_sem/TorchClassifier.ipynb#X16sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# делаем backward - вычисляются значения .grad у слоев модели\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/NLP_5_sem/TorchClassifier.ipynb#X16sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss_value\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32md:\\NLP_5_sem\\venv_nlp\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\NLP_5_sem\\venv_nlp\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:216\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 216\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnll_loss(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32md:\\NLP_5_sem\\venv_nlp\\Lib\\site-packages\\torch\\nn\\functional.py:2704\u001b[0m, in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2702\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2703\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 2704\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mnll_loss_nd(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "# цикл обучения (по эпохам)\n",
    "for epoch in range(epochs):\n",
    "    # за одну эпоху смотрим все батчи (по batch_size элементов)\n",
    "    for x_b, y_b in train_dl:\n",
    "        # делаем прямое распространение (получаем предсказание)\n",
    "        outputs = model(x_b)\n",
    "        # вычисляем значение функции потерь\n",
    "        loss_value = loss(outputs, y_b)\n",
    "        # делаем backward - вычисляются значения .grad у слоев модели\n",
    "        loss_value.backward()\n",
    "        # делаем шаг градиентного спуска с заданным у оптимизатора learning_rate\n",
    "        optimizer.step()\n",
    "        # зануляем .grad у слоев модели - для нового батча будем акумулировать новый .grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # в конце эпохи выводим значение функции потерь для последнего рассмотренного батча\n",
    "    print(f'Эпоха {epoch + 1}, Значение функции потерь: {loss_value.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
